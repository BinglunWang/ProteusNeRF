{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "from diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from transformers import pipeline\n",
    "from Model.triplanelite import triplane_fea\n",
    "from Dataloader.LLFF import LLFFDataset\n",
    "from Processing.trainer import *\n",
    "from Processing.rendering import novel_views_LLFF, creat_video, render_img\n",
    "from Processing.vis import load_settings, calc_query_emb, calc_feature_dist\n",
    "from Processing.editing import get_comb_img, save_history_fig, cp, \\\n",
    "    make_inpaint_condition, rgb2canny, mkdir_ifnoexit, context_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. general settings\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "scene = 'flower'\n",
    "load_features = False\n",
    "show_selection = False\n",
    "\n",
    "# 1. dataset settings\n",
    "datadir= os.path.join('./Dataset/nerf_llff_data', scene)\n",
    "fea_dir = None\n",
    "LLFF_training = LLFFDataset(datadir, fea_path = fea_dir, split='train',load_features=load_features, downsample=8)\n",
    "LLFF_test = LLFFDataset(datadir, fea_path = fea_dir, split='test',load_features=load_features, downsample=8)\n",
    "downsample = 8\n",
    "\n",
    "# 2. model settings\n",
    "pretrained_model_path = './pre_trained_models/' + scene + '.pth'\n",
    "aabb = torch.tensor([-1.7, 1.7])\n",
    "nerf_model = triplane_fea(aabb = aabb)\n",
    "nerf_model.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device(device)))\n",
    "nerf_model.to(device)\n",
    "\n",
    "# 3. other setting\n",
    "depth_estimator = pipeline('depth-estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Query a patch and select in rendering view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render a frame from the test dataset\n",
    "rgb_flower, emb_flower, depth_flower, mask_flower = render_img(nerf_model = nerf_model, device= device, Dataset = LLFF_test, img_index = 0, hn = 0, hf = 1, nb_bins = 96, req_others=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a patch from the rendered image and calculate the feature distance\n",
    "# modify from N3F official code, https://github.com/dichotomies/N3F\n",
    "settings = load_settings()[scene]\n",
    "factor = downsample\n",
    "r, c = settings['rc']\n",
    "extent = settings['sz']\n",
    "r = int(r * 8 / factor)\n",
    "c = int(c * 8 / factor)\n",
    "extent = int(extent * 8 / factor)\n",
    "img_w, img_h = LLFF_test.img_wh\n",
    "embq, dir_q = calc_query_emb(emb_flower, r, c, extent, rgb=rgb_flower, vis = True)\n",
    "dist = calc_feature_dist(embq, emb_flower)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(dist.view(-1).cpu().numpy(), bins=20, density=True, alpha=0.5, label='Ditilled Triplanes') \n",
    "plt.show()\n",
    "plt.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example: use the threshold dis_thr to get the mask of the selected region\n",
    "# modify from N3F official code, https://github.com/dichotomies/N3F\n",
    "rgb_j_fg, emb_j_fg, depth_j_fg, mask_j_fg = render_img(nerf_model = nerf_model, device= device, \\\n",
    "                            Dataset = LLFF_test, img_index = 0, hn = 0, hf = 1, nb_bins = 96,\\\n",
    "                            req_others = True, embq=embq, dis_thr=settings['thr'] + settings['margin'], \n",
    "                            foreground=False, show_selection=True)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(rgb_j_fg)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(depth_j_fg, cmap = 'gray')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mask_j_fg, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to see selection rendering results (videos) you can set show_selection to True:\n",
    "if show_selection:\n",
    "    foldername = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if not os.path.exists('./render_results'):\n",
    "        os.mkdir('./render_results')\n",
    "    folderpath = os.path.join('./render_results', foldername)\n",
    "    os.mkdir(folderpath)\n",
    "    \n",
    "    name = 'wo_selection'\n",
    "    novel_views_path = novel_views_LLFF(folderpath, name, nerf_model, device, LLFF_test, hn = 0, hf = 1, nb_bins = 96,\n",
    "                     req_others = True)\n",
    "    creat_video(novel_views_path, folderpath, name, req_others=False)\n",
    "    name = 'w_selection'\n",
    "    novel_views_path = novel_views_LLFF(folderpath, name, nerf_model, device, LLFF_test, hn = 0, hf = 1, nb_bins = 96,\n",
    "                     req_others = True, dis_thr = settings['thr'] + settings['margin'], \n",
    "                     embq = embq, dist_less=False, show_selection = True)\n",
    "    creat_video(novel_views_path, folderpath, name, req_others=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preview 3D-aware image context Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare 3D-aware image context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder to save editing results\n",
    "foldername = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "savepath_father = os.path.join(\"./editing_results\", foldername)\n",
    "mkdir_ifnoexit(\"./editing_results\")\n",
    "mkdir_ifnoexit(savepath_father)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get multiple contexts\n",
    "W, H = LLFF_training.img_wh\n",
    "n_imgs = len(LLFF_training) // W // H\n",
    "need_imgs = 8 # as discript in the paper, we need 8 rendered images to edit.\n",
    "max_iter = (need_imgs // 2 - 1) \n",
    "indices = np.random.choice(n_imgs, need_imgs, replace=False)\n",
    "repeat_indices = indices[:2]\n",
    "modified_indices = np.zeros(max_iter * 4, dtype=np.int16)\n",
    "modified_indices[:4] = indices[:4]\n",
    "for i in range(1, max_iter):\n",
    "    modified_indices[4*i: 4*i+2] = repeat_indices\n",
    "    modified_indices[4*i+2: 4*i+4] = indices[4+(2*(i-1)):4+(2*(i))]\n",
    "indices = modified_indices.astype(np.int16)\n",
    "\n",
    "mask_4_s_list = []\n",
    "gt_4_s_list = []\n",
    "rgb_4_s_list = []\n",
    "depth_4_s_list = []\n",
    "ray_4_s_list = []\n",
    "gt_4_list = []\n",
    "rgb_4_list = []\n",
    "depth_4_list = []\n",
    "ray_4_list = []\n",
    "\n",
    "for edit_iter in range(max_iter):\n",
    "    gt_4, rgb_4, depth_4, mask_4, ray_4 = get_comb_img(nerf_model, LLFF_training, device, indices[edit_iter * 4 : (edit_iter + 1) * 4])\n",
    "    gt_4_s, rgb_4_s, depth_4_s, mask_4_s, ray_4_s = get_comb_img(nerf_model, LLFF_training, device, indices[edit_iter * 4 : (edit_iter + 1) * 4], \n",
    "                                                    embq = embq, dis_thr = settings['thr'] + settings['margin'], show_selection = True)\n",
    "    \n",
    "    mask_4_s_list.append(mask_4_s)\n",
    "    gt_4_s_list.append(gt_4_s)\n",
    "    rgb_4_s_list.append(rgb_4_s)\n",
    "    depth_4_s_list.append(depth_4_s)\n",
    "    ray_4_s_list.append(ray_4_s)\n",
    "    gt_4_list.append(gt_4)\n",
    "    rgb_4_list.append(rgb_4)\n",
    "    depth_4_list.append(depth_4)\n",
    "    ray_4_list.append(ray_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Generative Image Editing Method - ContolNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 0.9 # Indicates extent to edit the object in NeRF\n",
    "wo_edge = False # Indicates extent to edit the object in NeRF, True edit more [W/ Canny edge information the object change \n",
    "prompt = \"purple flower made of origami paper, high quality, photo realistic\" # have fun to change it, we also provide some examples as follows\n",
    "#--------------------------------------------------------------- prompt examples ---------------------------------------------------------------\n",
    "# prompt = \"flower made of 24k gold, shiny, high quality, photo realistic\"\n",
    "# prompt = \"pink zinnia flower made of origami paper, high quality, photo realistic\"\n",
    "# prompt = \"flower made of paper, high quality, photo realistic\"\n",
    "# prompt = \"pink flower made of origami paper, high quality, photo realistic\"\n",
    "# prompt = \"orange zinnia flower with green pistil\"\n",
    "# prompt = \"purple flower made of origami paper, high quality, photo realistic\"\n",
    "# prompt = \"fire dragon, glowing lava\"\n",
    "# prompt = \"paper dragon\"\n",
    "# prompt = \"glowing ice dragon, blue\"\n",
    "# prompt = \"snow weather\"\n",
    "# prompt = \"vibrant green fern\"\n",
    "# prompt = \"horns of a dragon made of 24k shiny gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the generative image editor [ControlNet]\n",
    "controlnet = [\n",
    "    ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/control_v11p_sd15_inpaint\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ),\n",
    "    ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/control_v11f1p_sd15_depth\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ),\n",
    "]\n",
    "if wo_edge == False:\n",
    "    controlnet.append(\n",
    "        ControlNetModel.from_pretrained(\n",
    "            \"lllyasviel/control_v11p_sd15_canny\",\n",
    "            torch_dtype=torch.float16,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None,\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "generator = torch.Generator(device=device).manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show editing preview, if you are not satisfied with the result, you can change the prompt and run the following code again\n",
    "rgb_0 = rgb_4_list[0]\n",
    "mask_0 = mask_4_s_list[0]\n",
    "depth_0 = depth_4_list[0]\n",
    "ray_0 = ray_4_list[0]\n",
    "\n",
    "canny_0 = Image.fromarray(np.uint8(rgb2canny(cp(rgb_0))))\n",
    "if wo_edge == True:\n",
    "    control_image = [make_inpaint_condition(cp(rgb_0), cp(mask_0)), cp(depth_0)]\n",
    "else:\n",
    "    control_image = [make_inpaint_condition(cp(rgb_0), cp(mask_0)), cp(depth_0), canny_0]\n",
    "\n",
    "edited_0 = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    eta=1.0,\n",
    "    strength=strength,\n",
    "    image=cp(rgb_0),\n",
    "    mask_image=cp(mask_0),\n",
    "    control_image=control_image,\n",
    ").images[0]\n",
    "edited_0 = edited_0.resize((rgb_0.shape[1],rgb_0.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Edit object in NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit object parameters \n",
    "num_iters = 3 # editing iterations, please make sure the need_imgs is larger or equal than 2 * num_iters + 2\n",
    "lambda_depth = 1e-4 # depth loss weight, larger value reduce more floaters but make the density more sparse\n",
    "mask_trick = False # Reduce the floaters out of the object, True reduce floaters but increase the time cost (optional). In GTX 4090 Ti GPU, 59s -> 68s \n",
    "lr=2e-4 # editing training learning rate\n",
    "nb_epochs = 1 # editing training epochs\n",
    "record = True # False spend less time, record the editing history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = time.time()\n",
    "if mask_trick == True:\n",
    "    history, edit_nerfmodel = context_iter(nerf_model, savepath_father, LLFF_training, indices, edited_0, canny_0, rgb_4_list, \n",
    "                                        mask_4_s_list, depth_4_list, ray_4_list, pipe, prompt, generator, H, W,\n",
    "                                        depth_estimator, record=record, lr=lr, strength=strength, num_iters=num_iters, \n",
    "                                        nb_epochs=nb_epochs, lambda_depth = lambda_depth, mask_trick = mask_trick, original_nerf=nerf_model,\n",
    "                                       woedge = wo_edge, embq=embq, dis_thr=settings['thr'] + settings['margin'], foreground=True)\n",
    "else:\n",
    "    history, edit_nerfmodel = context_iter(nerf_model, savepath_father, LLFF_training, indices, edited_0, canny_0, rgb_4_list, \n",
    "                                        mask_4_s_list, depth_4_list, ray_4_list, pipe, prompt, generator, H, W,\n",
    "                                        depth_estimator, record=record, lr=lr, strength=strength, num_iters=num_iters, \n",
    "                                        nb_epochs=nb_epochs, lambda_depth = lambda_depth, woedge = wo_edge)\n",
    "T2 = time.time()\n",
    "print('Editing processing cost seconds: ', T2 - T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if record == True:\n",
    "    # show and save edited images, nerf novel views during iterations\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title('photo editing with iterations')\n",
    "    plt.axis('off')\n",
    "    for i in range(num_iters):\n",
    "        plt.subplot(1, num_iters, i + 1)\n",
    "        plt.imshow(history['photo'][i])\n",
    "        plt.axis('off')\n",
    "        plt.title(str(i + 1))\n",
    "    # get a time\n",
    "    plt.savefig(os.path.join(savepath_father, prompt + '_photo_iter.png'))\n",
    "    # plt.close()\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.title('nerf novel view with iterations')\n",
    "    plt.axis('off')\n",
    "    for i in range(num_iters):\n",
    "        plt.subplot(1, num_iters, i + 1)\n",
    "        plt.imshow(history['rendering'][i][1])\n",
    "        plt.axis('off')\n",
    "        plt.title(str(i + 1))\n",
    "    plt.savefig(os.path.join(savepath_father, prompt + '_nerf_iter.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render novel view results\n",
    "method = '_W_context'\n",
    "if record == True:\n",
    "    save_history_fig(savepath_father, history, prompt, method, num_iters)\n",
    "\n",
    "novel_views_path = novel_views_LLFF(savepath_father, method, edit_nerfmodel, device, LLFF_test, hn = 0, hf = 1, nb_bins = 96,\n",
    "                     req_others = False)\n",
    "creat_video(novel_views_path, savepath_father, method, req_others=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
